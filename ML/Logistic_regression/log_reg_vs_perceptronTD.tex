\documentclass[12pt]{article}

\usepackage{amsmath,amssymb,enumerate,graphicx,pgf,tikz,fancyhdr}
\usepackage[shortlabels]{enumitem}
\usepackage[utf8]{inputenc}
\usetikzlibrary{arrows}
\usepackage{geometry}
\usepackage{tabvar}
\usepackage{bbm}

\geometry{hmargin=2.2cm,vmargin=2.5cm}\pagestyle{fancy}
\rfoot{\bfseries\thepage}
\cfoot{}
\renewcommand{\footrulewidth}{1.8pt}
\renewcommand{\headrulewidth}{1.8pt}
\input{macros}

\makeatletter
\patchcmd{\f@nch@head}{\rlap}{\color{primaryColor}\rlap}{}{}
\patchcmd{\headrule}{\hrule}{\color{primaryColor}\hrule}{}{}
\patchcmd{\f@nch@foot}{\rlap}{\color{primaryColor}\rlap}{}{}
\patchcmd{\footrule}{\hrule}{\color{primaryColor}\hrule}{}{}
\makeatother


\usepackage[mathscr]{euscript}
\usepackage{amsmath}

\begin{document} 

\begin{titlepage}
\lhead{\textcolor{primaryColor}{ML M1-IDD - 2020/2021}}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} 

\center
 
%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------


\includegraphics[scale=0.6]{upd.png}
\center\textsc{\large M1-I2D}\\[0.5cm]
\vspace{2cm}
\textsc{\Large \textcolor{primaryColor}{TD 3 :} Machine Learning}\\[1.5cm] 


%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[0.4cm]
{ \huge \bfseries Logistic Regression}\\[0.4cm] 
\HRule \\[1.5cm]
 
%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
Benamara Abdelkader Chihab 
\end{flushleft}
\end{minipage}
\vspace{2cm}

%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------


{\large February 07, 2021}\\[2cm]



\vfill 

\end{titlepage}

\Exo{1}{Regression Logistique vs Perceptron}
Soit les exemples $x_i,y_i$ avec $x_i \in \R^d$ et $y_i \in \{0,1\}$ on a la fonction cout définie avec :
\begin{equation}
    Cout_c(\theta,x_i,y_i) = y_i \max(0,c-\theta^Tx_i) + (1-y_i) \max(0,c+\theta^Tx_i)
    \label{eq:e1}
\end{equation}
\begin{enumerate}[1)]
    \item ($c=0$) Calcul du gradient de $Cout$ par rapport à $\theta$ : 
     \\
     On peut écrire la formule de la fonction $Cout_0$ de la maniere équivalente suivante :
     $$
     Cout_0(\theta,x_i,y_i) =
     \begin{cases}
         (1-y_i)\mbox{ }\theta^Tx_i, & \mbox{si } \theta^Tx_i \geq 0 \\
         -y_i\theta^Tx_i, & \mbox{sinon } 
     \end{cases}
     $$\\
     Et donc $\nabla_\theta Cout_0$ est défini par : 
          $$
     \nabla_\theta Cout_0(\theta,x_i,y_i) =
     \begin{cases}
         (1-y_i) x_i, & \mbox{si } \theta^Tx_i \geq 0 \\
         -y_i x_i, & \mbox{sinon } 
     \end{cases}
     $$
     \item Soit $J_0(\theta) = \Sigma_{i=1}^N Cout_0(\theta,x_i,y_i)$ , l'algorithme de descente du gradient stochastique est donné par : \\
    \begin{algorithm}[H]
    \SetAlgoLined
     \textbf{Input :} la fonction $J_0(\theta) = \Sigma_{i=1}^N Cout_0(\theta,x_i,y_i)$ qui resprecte bien la forme vu en cours. Et un pas $\alpha$ \\
    \textbf{Output :} Une valeur $\theta$ qui minimise $J_0$\\
     \Begin{
      $\theta = 0_{\R^d}$\;
     \Repeter{ $\norm{g} \geq \varepsilon$ }
     {
        Tirer $i$ uniformément de 1 à $N$ \;
        $v$ = $\theta^Tx_i$ \;
        \Si{( $v \geq 0 $ )}
        {
            $g$ =$(1-y_i) x_i $ \;
        }
        \Sinon
        {
            $g$ =$-y_i x_i $ \;
        }
        $\theta \leftarrow \theta - \alpha g$ \; 
      }  
      \Return $\theta$
      }
      \caption{SGD -- pour la fonction $J_0$}
    \end{algorithm}\leavevmode\newline 
    \clearpage
    \item
    On fait analogie avec l'algorithme d'apprentissage du perceptron en version stochastique , on distingue deux cas principaux pour la valeur de $\theta$ Soit elle sera modifiée ( les poids du perceptron $\theta$ sont poussés dans la direction de la classe correcte dans l'itération suivante ou pas de modification des poids $\theta$ du perceptron ( si on a pas commis d'erreur).
    \item ($c=1$) Soit la fonction de cout $J_1$ : $J_1(\theta) = \Sigma_{i=1}^N Cout_1(\theta,x_i,y_i)$ , et le classifieur $f_\theta$ associé à $\theta$ $
     f_\theta(x) =
     \begin{cases}
         1 & \mbox{si } \theta^Tx \geq 0 \\
         0 & \mbox{sinon } 
     \end{cases}
     $ et la fonction d'erreur $err$ donnée par : \\ 
     $err(\theta) = \Sigma_{i=1}^N \textbf{1}[f_\theta(x_i)\neq y_i]$ . Montrons que $err(\theta) \leq J_1(\theta)$ pour tout $\theta \in \R^d$.\\
     La fonction $Cout_1$ est donnée par la relation : 
     \begin{equation}
        Cout_1(\theta,x_i,y_i) = y_i \max(0,1-\theta^Tx_i) + (1-y_i) \max(0,1+\theta^Tx_i)
        \label{eq:e1}
    \end{equation}
     On ecrit la fonction $Cout_1$ de cette maniere :
     $$
     Cout_1(\theta,x_i,y_i) =
     \begin{cases}
         y_i(1-\theta^Tx_i), & \mbox{si } \theta^Tx_i < -1 \\
         1+(1-2y_i)\theta^Tx_i, & \mbox{si } -1 \leq \theta^Tx_i < 1 \\
         (1-y_i)(1+\theta^Tx_i), & \mbox{si }   \theta^Tx_i \geq 1 \\
     \end{cases}
     $$\\
     On a d'autres part
     $$
     \textbf{1}[f_\theta(x_i)\neq y_i] = \textbf{1}[f_\theta(x_i) = 1 \land y_i = 0 ]+ \textbf{1}[f_\theta(x_i) = 0 \land y_i = 1 ]
     $$
     $$
     = \textbf{1}[\theta^Tx_i \geq 0 \land y_i = 0 ]+ \textbf{1}[\theta^Tx_i < 0 \land y_i = 1 ]
     $$
     Dans le cas ou ($\theta^Tx_i \geq 0 \land y_i = 0$) on a $Cout_1(\theta,x_i,y_i) = 1+\theta^Tx_i \geq 1$ \\
     Et dans l'autre cas ou ($\theta^Tx_i < 0 \land y_i = 1$) on a $Cout_1(\theta,x_i,y_i) = 1-\theta^Tx_i \geq 1$ \\
     Donc dans les deux cas possibles on a $err(\theta) \leq J_1(\theta)$ ( On somme et on minore chaque terme ).\\
     On en déduit que l'optimisation de $J_1(\theta)$ est un probleme difficile puisque la minimisation de $err(\theta)$ est difficile.
\end{enumerate}
\end{document}



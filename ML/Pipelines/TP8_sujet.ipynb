{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP8_sujet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdVERcgmkUQB"
      },
      "source": [
        "**Import des libraires**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwR3nO-UkMNQ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import RFE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZdHB3JAkavH"
      },
      "source": [
        "Dans ce TP, nous allons travailleurs sur les données *Titanic*. Ces données contiennent des variables décrivant les passagers du Titanic et la variable d'intérêt que l'on va chercher à prédire est celle qui indique leur survie (apellée *Survived* dans le fichier).\n",
        "\n",
        "Pour plus de précisions sur ces données, on pourra consulter : \n",
        "https://www.kaggle.com/c/titanic/data\n",
        "\n",
        "\n",
        "1. Dans un premier temps, on va regarder le type de chacunes des variables et si certaines observations sont manquantes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhFkSfmnkd--"
      },
      "source": [
        "titanic = pd.read_csv('https://gist.githubusercontent.com/michhar/2dfd2de0d4f8727f873422c5d959fff5/raw/fa71405126017e6a37bea592440b4bee94bf7b9e/titanic.csv')\n",
        "\n",
        "X = titanic.drop('Survived', axis = 1)\n",
        "y = titanic.Survived\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd5gChvlkfmj"
      },
      "source": [
        "# types de variables\n",
        "# données manquantes \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjrWg3nfke7a"
      },
      "source": [
        "Comme indiqué précédemment, on cherche à prédire la classe (*Survived*) en fonction des autres variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IChhgcVskzfp"
      },
      "source": [
        "X = titanic.drop('Survived', axis = 1)\n",
        "y = titanic.Survived"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8Cgpd4yk2Ew"
      },
      "source": [
        "On va tout d'abord s'intéresser aux données numériques.\n",
        "\n",
        "2. Pour gérer le problème des valeurs manquantes, effectuer une imputation (par la moyenne ou la médiane) en utilisant *SimpleImputer* puis appliquer une régression logistique.\n",
        "3. Après cette imputation, on va standardiser les données via *StandardScaler*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBCj7tn4k9Di"
      },
      "source": [
        "# seperation des variables suivant leurs types \n",
        "numeric_features = ['Age', 'SibSp', 'Parch', 'Fare']\n",
        "categorical_features = ['Pclass', 'Sex', 'Embarked']\n",
        "\n",
        "X_num= X[numeric_features].copy()\n",
        "\n",
        "X_train, X_holdout, y_train, y_holdout = train_test_split(X_num, y, stratify = y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "\n",
        "#imputation des données manquantes \n",
        "\n",
        "# standardisation des données \n",
        "\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred= clf.predict(X_holdout)\n",
        "\n",
        "sum(y_pred==y_holdout)/len(y_holdout)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_PRnEk1lMzA"
      },
      "source": [
        "On va tout maintenant s'intéresser aux données catégorielles.\n",
        "\n",
        "4. Pour gérer le problème des valeurs manquantes, effectuer une imputation (en utilisant les bons paramètres pour ce type de données).\n",
        "5. Après cette imputation, on va appliquera un *One Hot Encoding* les données avant d'appliquer une simple régression logistique."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffxeaxhMlSbB"
      },
      "source": [
        "X_cat= X[categorical_features].copy()\n",
        "\n",
        "X_train, X_holdout, y_train, y_holdout = train_test_split(X_cat, y, stratify = y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "#imputation \n",
        "\n",
        "\n",
        "#one hot encoder\n",
        "\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred= clf.predict(X_holdout)\n",
        "\n",
        "sum(y_pred==y_holdout)/len(y_holdout)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fssMHlOAl5yF"
      },
      "source": [
        "Les pre-processing utilisés ainsi que le classifieur ont tous des paramètres qu'il conviendrait de régler correctement. \n",
        "\n",
        "6. Reprenons notre chaîne de traitement des données numériques et cette fois, nous allons créer un objet *Pipeline* qui contiendra nos étapes d'imputation, de standardisation et de classification.\n",
        "On cherchera à trouver la meilleure valeur C (pour la régression logisitque sur la grille [0.01, 0.1, 1, 10, 100]), si la standardisation est nécessaire ou pas et enfin si il est préférable d'utiliser la moyenne ou la médiane pour l'imputation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdiKjuvDl6wE"
      },
      "source": [
        "# donnees numériques\n",
        "\n",
        "X_train, X_holdout, y_train, y_holdout = train_test_split(X_num, y, stratify = y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "\n",
        "# Pipeline : imputer -> scaler -> classifieur\n",
        "# dans un pipeline, un élément doit avoir des fonctions fit et transform\n",
        "# la fonction fit est appelée et les données transformées sont données en entrée de l'élement suivant\n",
        "# ex : pipeline = Pipeline ([ ('preproc', normalize()), ('clf', LogisticRegression( ))])\n",
        "\n",
        "pipeline \n",
        "\n",
        "\n",
        "# les paramètres p d'un élement e du pipeline est identifié par la chaine de caractères 'e__p\" et ensuite un nparray précise les valeurs possibles\n",
        "# ex : params = [{'clf__C': [0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "\n",
        "params \n",
        "\n",
        "\n",
        "\n",
        "rskf = StratifiedKFold(n_splits=10, random_state=42, shuffle=False)\n",
        "cv = GridSearchCV(pipeline, params, cv = rskf, scoring = 'accuracy', n_jobs = -1)\n",
        "\n",
        "cv.fit(X_train, y_train)\n",
        "\n",
        "print(f'Best accuracy -score: {cv.best_score_:.3f}\\n')\n",
        "print(f'Best parameter set: {cv.best_params_}\\n')\n",
        "print(f'Scores: {classification_report(y_train, cv.predict(X_train))}')\n",
        "\n",
        "preds = cv.predict(X_holdout)\n",
        "print(f'Scores: {classification_report(y_holdout, preds)}\\n')\n",
        "print(f'accuracy score: {accuracy_score(y_holdout, preds):.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go4ZTrYuoEuE"
      },
      "source": [
        "Maintent que l'on sait créer et valider un pipeline, on va voir que ceux-ci peuvent se combiner.\n",
        "\n",
        "5. A l'aide de *ColumnTransformer*, on va appliquer les pipelins précédents sur chaque sous-ensemble de variables (numériques et catégorielles) et les combiner. Cette objet sera alors mis lui-même dans un pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDYoYcxToFkz"
      },
      "source": [
        "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, stratify = y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "# pipeline (preprocessing uniquement) sur les donnes catégorielles\n",
        "categorical_features = ['Pclass', 'Sex', 'Embarked']\n",
        "categorical_transformer = Pipeline\n",
        "\n",
        "# pipeline (preprocessing uniquement) sur les données numériques \n",
        "numeric_features = ['Age', 'SibSp', 'Parch', 'Fare']\n",
        "numeric_transformer = Pipeline(\n",
        "    [\n",
        "        ('imputer_num', SimpleImputer()),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]\n",
        ")\n",
        "\n",
        "# chaque pipeline s'applique sur un sous-ensemble des variables\n",
        "preprocessor = ColumnTransformer(\n",
        "    [\n",
        "        ('categoricals', categorical_transformer, categorical_features),\n",
        "        ('numericals', numeric_transformer, numeric_features)\n",
        "    ],\n",
        "    remainder = 'drop'\n",
        ")\n",
        "\n",
        "# on crée un pipeline combinat l'objet preprocessor et un classifieur\n",
        "pipeline \n",
        "\n",
        "\n",
        "params \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "rskf = StratifiedKFold(n_splits = 10)\n",
        "\n",
        "cv = GridSearchCV(pipeline, params, cv = rskf, scoring = 'accuracy', n_jobs = -1)\n",
        "\n",
        "cv.fit(X_train, y_train)\n",
        "\n",
        "print(f'Best accuracy-score: {cv.best_score_:.3f}\\n')\n",
        "print(f'Best parameter set: {cv.best_params_}\\n')\n",
        "print(f'Scores: {classification_report(y_train, cv.predict(X_train))}')\n",
        "\n",
        "preds = cv.predict(X_holdout)\n",
        "print(f'Scores: {classification_report(y_holdout, preds)}\\n')\n",
        "print(f'accuracy-score: {accuracy_score(y_holdout, preds):.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ALP2KzEpUkb"
      },
      "source": [
        "Pour aller plus loin, on pourra introduire une étape de RFE dans le pipeline précédent.\n",
        "\n",
        "On pourra aussi appliquer cette méthodologie sur d'autres données (winequality-red.csv ou bien IMDB 5000 Movie)\n"
      ]
    }
  ]
}